% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wlr.power.maxcombo0.R
\name{wlr.power.maxcombo0}
\alias{wlr.power.maxcombo0}
\title{Power Calculation at a Calendar Time Using A Max-combo Test}
\usage{
wlr.power.maxcombo0(
  T = 36,
  events = NULL,
  alpha = 0.025,
  power = 0.9,
  side = 1,
  r = 1,
  n = NULL,
  h0 = function(t) {     log(2)/12 },
  S0 = function(t) {     exp(-log(2)/12 * t) },
  h1 = function(t) {     log(2)/12 * 0.7 },
  S1 = function(t) {     exp(-log(2)/12 * 0.7 * t) },
  f.logHR = function(t) {     log(as.numeric(t < 6) + as.numeric(t >= 6) * 0.65) },
  f.ws = list(lr, fh01, fh11),
  F.entry = function(t) {     (t/18) * as.numeric(t <= 18) + as.numeric(t > 18) },
  G.ltfu = function(t) {     0 },
  non.centrality = "Heetal2021"
)
}
\arguments{
\item{T}{Calendar time for analysis}

\item{events}{Target number of events}

\item{alpha}{Allocated alpha level. Default 0.025 (1-sided).}

\item{power}{Power, default 0.9}

\item{side}{Side of test 1 or 2. Default side = 1.}

\item{r}{Randomization ratio of experimental arm : control arm as r:1. When r = 1, it is equal allocation. Default r = 1.}

\item{n}{Total sample size for two arms. Default n = NULL. If n is not provided,
n will be calculated based on single-time point analysis using
the first weighted log-rank test as a starting point with alpha level.}

\item{h0}{Hazard function of control arm. h0(t) = log(2)/m0 means T~exponential distribution with median m0.}

\item{S0}{Survival function of control arm. In general, S0(t) = exp(- integral of h0(u) for u from 0 to t).
but providing S0(t) can improves computational efficiency and
usually the survival function is known in study design. The density function f0(t) = h0(t) * S0(t).}

\item{h1}{Hazard function of experimental arm. h1(t) = log(2)/m1 means T~exponential distribution with median m0.}

\item{S1}{Survival function of experimental arm. In general, S1(t) = exp(- integral of h1(u) for u from 0 to t).
but providing S1(t) can improves computational efficiency and
usually the survival function is known in study design. The density function f1(t) = h1(t) * S1(t).}

\item{f.logHR}{Log(hazard ratio) function, = h1(t) / h0(t).}

\item{f.ws}{Self-defined weight function of survival rate.
For example, f.ws = function(s){1/max(s, 0.25)}
When f.ws is specified, the weight function takes them as priority.\preformatted{    Either f.ws or (rho, gamma, tau, s.tau) must be specified. For K analyses,
    must provide f.ws as a list of K-component weight functions or 
    (rho, gamma, tau, s.tau) with K components for each parameter.
    
    Note: The actual math formula for weighting function is based on 
    the left of each event time t, i.e., w(t) = f.ws(s(t-)). 
    For FH(rho, gamma) test, when gamma = 0, then the first event time has 
    weight 1;  when gamma > 0, the first event weight is 0. This can 
    ensure consistency with FH(0,0) = logrank; and FH(1,0) = generalized Wilcoxon.
}}

\item{F.entry}{Distribution function of enrollment. For example,
F.entry(t) = (t/A)^psi*I(0<=t<=A) + I(t>A).
Default F.entry is uniform distribution function.}

\item{G.ltfu}{Distribution function of lost-to-follow-up censoring process. The observed
survival time is min(survival time, lost-to-follow-up time).
Default G.ltfu = 0 (no lost-to-followup)}

\item{non.centrality}{Option of non-centrality parameter either "Heetal2021"
or "Schoenfeld". Default "Heetal2021". The default method is more efficient
for HR < 0.65 in non-proportional hazards scenarios. No difference when HR > 0.65.}

\item{rho}{Vector of rho parameters for multiple Fleming-Harrington (rho, gamma)
weighted log-rank tests.}

\item{gamma}{Vector of parameters for Fleming-Harrington (rho, gamma)
weighted log-rank tests. For log-rank test, set rho = gamma = 0.}

\item{tau}{Cut point for stabilized FH test, sFH(rho, gamma, tau); with weight
function defined as w(t) = s_tilda^rho*(1-s_tilda)^gamma, where
s_tilda = max(s(t), s.tau) or max(s(t), s(tau)) if s.tau = NULL
tau = Inf reduces to regular Fleming-Harrington test(rho, gamma)}

\item{s.tau}{Survival rate cut S(tau) at t = tau1; default 0.5, ie. cut at median.
s.tau = 0 reduces to regular Fleming-Harrington test(rho, gamma)}
}
\value{
An object with dataframes below.
\itemize{
\item  mu: Non-centrality parameter for each weighted log-rank test included in the max-combo.
\item  events: Expected number of events at the calendar time T
\item  T:      Expected calendar time for analysis (Data Cutoff Date)
\item  power:  Power of the max-combo test
\item  power.piece Power of each weighted log-rank test included in the max-combo test.
\item  n:       Total number of subjects for two arms
\item  medians: Median of each treatment group
\item  corr.Hp: Correlation matrix of the weighted log-rank tests included
in the max-combo test for the pooled distribution (H0).
\item  corr.strict.H0 Correlation matrix of the weighted log-rank tests included
in the max-combo test assuming the data strictly follow
the control arm distribution (Strict H0).
\item  wt: Weight functions used for the weighted log-rank tests included
in the max-combo test.
\item  bounds: Rejection boundary for the max-combo test after multiplicity
adjustment based on asymptotic multivariate normal distribution.
b.Hp: bound based on pooled distribution (preferred);
b.H0: bound based on strict H0
\item  setting Setting of the distribution for each arm (hazard function,
survival function, and log(HR) function), enrollment distribution
function, and lost-to-followup distribution.
\item  non-centrality: Method of non-centrality parameter
}
}
\description{
This function calculates the power at a calendar time based on the asymptotic
distribution of the weighted log-rank test statistic under H1, with provided
rejection boundaries.
For group sequential design, the power will be calculated for each analysis
and overall study.

The non-centrality parameter can be chosen from Schoenfeld (1981) method, or (He et al 2021) method (default)
The difference is usually negligible under most common situations with HR large.
However, when the assumed HR under H1 is small for weighted log-rank test, there is
notable small difference, in which scenario (He et al 2021) method can potentially
reduce sample size. Regarding the timing of analyses, either the calendar time
or required number of total events can be specified. By default, the required
number of total events will be used as determination of analysis cut off.
This function allows flexible alternative hypothesis in terms of HR(t), the
hazard ratio function over time. For delayed effect scenario under H1,
one can define HR(t) as a piecewise constant function of survival time t.
In addition, the function can handle user-defined flexible non-uniform enrollment
distribution function and independent time to lost-to-followup process which
is user-defined function of any lost-to-followup pattern such as constant
lost-to-followup rate or Weibull distribution. For most common setting
in practice, assuming the same lost-to-followup pattern in both arms.
}
\examples{
#Distributions for both arms
m0 = 12; #median RFS for control arm
lambda0 = log(2) / m0
h0 = function(t){lambda0}; 
S0 = function(t){exp(-lambda0 * t)}
HRd = 0.60 #hazard ratio after delay

h.D3=function(t){lambda0*as.numeric(t<3)+HRd*lambda0*as.numeric(t>=3)}
c3 = exp(-3*lambda0*(1-HRd)); 
S.D3 = function(t){S0(t)*as.numeric(t<3)+c3*exp(-HRd*lambda0*t)*as.numeric(t>=3)}
f.logHR.D3 = function(t){log(as.numeric(t<3) + as.numeric(t>= 3)*HRd)}

h.D6=function(t){lambda0*as.numeric(t<6)+HRd*lambda0*as.numeric(t>=6)}
c6 = exp(-6*lambda0*(1-HRd)); 
S.D6 = function(t){exp(-lambda0*t)*as.numeric(t<6)+c6*exp(-HRd*lambda0*t)*as.numeric(t>=6)}
f.logHR.D6 = function(t){log(as.numeric(t<6) + as.numeric(t>= 6)*HRd)}

#Define weight funtions for weighted log-rank tests
lr = function(s){1}
fh01 = function(s){(1-s)}
fh11 = function(s){s*(1-s)}
#stabilized FH(0, 1; 0.5)
sfh01 = function(s){s1 = apply(cbind(s, 0.5), MARGIN=1,FUN=max); return(1-s1)} 
#modestly log-rank
mfh01 = function(s){s1 = apply(cbind(s, 0.5), MARGIN=1,FUN=max); return(1/s1)}

#Define max-combo test
maxcombo1 = list(lr)
maxcombo2 = list(lr, fh01)
maxcombo3 = list(lr, fh01, fh11)

maxcombo4 = list(lr, sfh01)
maxcombo5 = list(lr, mfh01)
maxcombo6 = list(lr, sfh01, mfh01, fh01)

wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.05), 
  power = 0.9, side = 2, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo1)

wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.025), 
  power = 0.9, side = 1, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo2)
  
wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.025), 
  power = 0.9, side = 1, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo3)
  
wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.025), 
  power = 0.9, side = 1, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo4)
  
wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.025), 
  power = 0.9, side = 1, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo5)
  
wlr.power.maxcombo0(T = 36, events = NULL, alpha=c(0.025), 
  power = 0.9, side = 1, r = 1, n = NULL, 
  h0 = h0, S0=S0,h1 = h.D3, S1= S.D3, f.logHR = f.logHR.D3, f.ws = maxcombo6)
  
}
